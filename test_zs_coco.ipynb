{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpeg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\"]).to(device) # n,d\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('/home/mila/l/le.zhang/scratch/light_align/output/raw_data_linear/model_14.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['text_model.model.embeddings.word_embeddings.weight', 'text_model.model.embeddings.position_embeddings.weight', 'text_model.model.embeddings.LayerNorm.weight', 'text_model.model.embeddings.LayerNorm.bias', 'text_model.model.encoder.layer.0.attention.attn.q.weight', 'text_model.model.encoder.layer.0.attention.attn.q.bias', 'text_model.model.encoder.layer.0.attention.attn.k.weight', 'text_model.model.encoder.layer.0.attention.attn.k.bias', 'text_model.model.encoder.layer.0.attention.attn.v.weight', 'text_model.model.encoder.layer.0.attention.attn.v.bias', 'text_model.model.encoder.layer.0.attention.attn.o.weight', 'text_model.model.encoder.layer.0.attention.attn.o.bias', 'text_model.model.encoder.layer.0.attention.LayerNorm.weight', 'text_model.model.encoder.layer.0.attention.LayerNorm.bias', 'text_model.model.encoder.layer.0.intermediate.dense.weight', 'text_model.model.encoder.layer.0.intermediate.dense.bias', 'text_model.model.encoder.layer.0.output.dense.weight', 'text_model.model.encoder.layer.0.output.dense.bias', 'text_model.model.encoder.layer.0.output.LayerNorm.weight', 'text_model.model.encoder.layer.0.output.LayerNorm.bias', 'text_model.model.encoder.layer.1.attention.attn.q.weight', 'text_model.model.encoder.layer.1.attention.attn.q.bias', 'text_model.model.encoder.layer.1.attention.attn.k.weight', 'text_model.model.encoder.layer.1.attention.attn.k.bias', 'text_model.model.encoder.layer.1.attention.attn.v.weight', 'text_model.model.encoder.layer.1.attention.attn.v.bias', 'text_model.model.encoder.layer.1.attention.attn.o.weight', 'text_model.model.encoder.layer.1.attention.attn.o.bias', 'text_model.model.encoder.layer.1.attention.LayerNorm.weight', 'text_model.model.encoder.layer.1.attention.LayerNorm.bias', 'text_model.model.encoder.layer.1.intermediate.dense.weight', 'text_model.model.encoder.layer.1.intermediate.dense.bias', 'text_model.model.encoder.layer.1.output.dense.weight', 'text_model.model.encoder.layer.1.output.dense.bias', 'text_model.model.encoder.layer.1.output.LayerNorm.weight', 'text_model.model.encoder.layer.1.output.LayerNorm.bias', 'text_model.model.encoder.layer.2.attention.attn.q.weight', 'text_model.model.encoder.layer.2.attention.attn.q.bias', 'text_model.model.encoder.layer.2.attention.attn.k.weight', 'text_model.model.encoder.layer.2.attention.attn.k.bias', 'text_model.model.encoder.layer.2.attention.attn.v.weight', 'text_model.model.encoder.layer.2.attention.attn.v.bias', 'text_model.model.encoder.layer.2.attention.attn.o.weight', 'text_model.model.encoder.layer.2.attention.attn.o.bias', 'text_model.model.encoder.layer.2.attention.LayerNorm.weight', 'text_model.model.encoder.layer.2.attention.LayerNorm.bias', 'text_model.model.encoder.layer.2.intermediate.dense.weight', 'text_model.model.encoder.layer.2.intermediate.dense.bias', 'text_model.model.encoder.layer.2.output.dense.weight', 'text_model.model.encoder.layer.2.output.dense.bias', 'text_model.model.encoder.layer.2.output.LayerNorm.weight', 'text_model.model.encoder.layer.2.output.LayerNorm.bias', 'text_model.model.encoder.layer.3.attention.attn.q.weight', 'text_model.model.encoder.layer.3.attention.attn.q.bias', 'text_model.model.encoder.layer.3.attention.attn.k.weight', 'text_model.model.encoder.layer.3.attention.attn.k.bias', 'text_model.model.encoder.layer.3.attention.attn.v.weight', 'text_model.model.encoder.layer.3.attention.attn.v.bias', 'text_model.model.encoder.layer.3.attention.attn.o.weight', 'text_model.model.encoder.layer.3.attention.attn.o.bias', 'text_model.model.encoder.layer.3.attention.LayerNorm.weight', 'text_model.model.encoder.layer.3.attention.LayerNorm.bias', 'text_model.model.encoder.layer.3.intermediate.dense.weight', 'text_model.model.encoder.layer.3.intermediate.dense.bias', 'text_model.model.encoder.layer.3.output.dense.weight', 'text_model.model.encoder.layer.3.output.dense.bias', 'text_model.model.encoder.layer.3.output.LayerNorm.weight', 'text_model.model.encoder.layer.3.output.LayerNorm.bias', 'text_model.model.encoder.layer.4.attention.attn.q.weight', 'text_model.model.encoder.layer.4.attention.attn.q.bias', 'text_model.model.encoder.layer.4.attention.attn.k.weight', 'text_model.model.encoder.layer.4.attention.attn.k.bias', 'text_model.model.encoder.layer.4.attention.attn.v.weight', 'text_model.model.encoder.layer.4.attention.attn.v.bias', 'text_model.model.encoder.layer.4.attention.attn.o.weight', 'text_model.model.encoder.layer.4.attention.attn.o.bias', 'text_model.model.encoder.layer.4.attention.LayerNorm.weight', 'text_model.model.encoder.layer.4.attention.LayerNorm.bias', 'text_model.model.encoder.layer.4.intermediate.dense.weight', 'text_model.model.encoder.layer.4.intermediate.dense.bias', 'text_model.model.encoder.layer.4.output.dense.weight', 'text_model.model.encoder.layer.4.output.dense.bias', 'text_model.model.encoder.layer.4.output.LayerNorm.weight', 'text_model.model.encoder.layer.4.output.LayerNorm.bias', 'text_model.model.encoder.layer.5.attention.attn.q.weight', 'text_model.model.encoder.layer.5.attention.attn.q.bias', 'text_model.model.encoder.layer.5.attention.attn.k.weight', 'text_model.model.encoder.layer.5.attention.attn.k.bias', 'text_model.model.encoder.layer.5.attention.attn.v.weight', 'text_model.model.encoder.layer.5.attention.attn.v.bias', 'text_model.model.encoder.layer.5.attention.attn.o.weight', 'text_model.model.encoder.layer.5.attention.attn.o.bias', 'text_model.model.encoder.layer.5.attention.LayerNorm.weight', 'text_model.model.encoder.layer.5.attention.LayerNorm.bias', 'text_model.model.encoder.layer.5.intermediate.dense.weight', 'text_model.model.encoder.layer.5.intermediate.dense.bias', 'text_model.model.encoder.layer.5.output.dense.weight', 'text_model.model.encoder.layer.5.output.dense.bias', 'text_model.model.encoder.layer.5.output.LayerNorm.weight', 'text_model.model.encoder.layer.5.output.LayerNorm.bias', 'text_model.model.encoder.layer.6.attention.attn.q.weight', 'text_model.model.encoder.layer.6.attention.attn.q.bias', 'text_model.model.encoder.layer.6.attention.attn.k.weight', 'text_model.model.encoder.layer.6.attention.attn.k.bias', 'text_model.model.encoder.layer.6.attention.attn.v.weight', 'text_model.model.encoder.layer.6.attention.attn.v.bias', 'text_model.model.encoder.layer.6.attention.attn.o.weight', 'text_model.model.encoder.layer.6.attention.attn.o.bias', 'text_model.model.encoder.layer.6.attention.LayerNorm.weight', 'text_model.model.encoder.layer.6.attention.LayerNorm.bias', 'text_model.model.encoder.layer.6.intermediate.dense.weight', 'text_model.model.encoder.layer.6.intermediate.dense.bias', 'text_model.model.encoder.layer.6.output.dense.weight', 'text_model.model.encoder.layer.6.output.dense.bias', 'text_model.model.encoder.layer.6.output.LayerNorm.weight', 'text_model.model.encoder.layer.6.output.LayerNorm.bias', 'text_model.model.encoder.layer.7.attention.attn.q.weight', 'text_model.model.encoder.layer.7.attention.attn.q.bias', 'text_model.model.encoder.layer.7.attention.attn.k.weight', 'text_model.model.encoder.layer.7.attention.attn.k.bias', 'text_model.model.encoder.layer.7.attention.attn.v.weight', 'text_model.model.encoder.layer.7.attention.attn.v.bias', 'text_model.model.encoder.layer.7.attention.attn.o.weight', 'text_model.model.encoder.layer.7.attention.attn.o.bias', 'text_model.model.encoder.layer.7.attention.LayerNorm.weight', 'text_model.model.encoder.layer.7.attention.LayerNorm.bias', 'text_model.model.encoder.layer.7.intermediate.dense.weight', 'text_model.model.encoder.layer.7.intermediate.dense.bias', 'text_model.model.encoder.layer.7.output.dense.weight', 'text_model.model.encoder.layer.7.output.dense.bias', 'text_model.model.encoder.layer.7.output.LayerNorm.weight', 'text_model.model.encoder.layer.7.output.LayerNorm.bias', 'text_model.model.encoder.layer.8.attention.attn.q.weight', 'text_model.model.encoder.layer.8.attention.attn.q.bias', 'text_model.model.encoder.layer.8.attention.attn.k.weight', 'text_model.model.encoder.layer.8.attention.attn.k.bias', 'text_model.model.encoder.layer.8.attention.attn.v.weight', 'text_model.model.encoder.layer.8.attention.attn.v.bias', 'text_model.model.encoder.layer.8.attention.attn.o.weight', 'text_model.model.encoder.layer.8.attention.attn.o.bias', 'text_model.model.encoder.layer.8.attention.LayerNorm.weight', 'text_model.model.encoder.layer.8.attention.LayerNorm.bias', 'text_model.model.encoder.layer.8.intermediate.dense.weight', 'text_model.model.encoder.layer.8.intermediate.dense.bias', 'text_model.model.encoder.layer.8.output.dense.weight', 'text_model.model.encoder.layer.8.output.dense.bias', 'text_model.model.encoder.layer.8.output.LayerNorm.weight', 'text_model.model.encoder.layer.8.output.LayerNorm.bias', 'text_model.model.encoder.layer.9.attention.attn.q.weight', 'text_model.model.encoder.layer.9.attention.attn.q.bias', 'text_model.model.encoder.layer.9.attention.attn.k.weight', 'text_model.model.encoder.layer.9.attention.attn.k.bias', 'text_model.model.encoder.layer.9.attention.attn.v.weight', 'text_model.model.encoder.layer.9.attention.attn.v.bias', 'text_model.model.encoder.layer.9.attention.attn.o.weight', 'text_model.model.encoder.layer.9.attention.attn.o.bias', 'text_model.model.encoder.layer.9.attention.LayerNorm.weight', 'text_model.model.encoder.layer.9.attention.LayerNorm.bias', 'text_model.model.encoder.layer.9.intermediate.dense.weight', 'text_model.model.encoder.layer.9.intermediate.dense.bias', 'text_model.model.encoder.layer.9.output.dense.weight', 'text_model.model.encoder.layer.9.output.dense.bias', 'text_model.model.encoder.layer.9.output.LayerNorm.weight', 'text_model.model.encoder.layer.9.output.LayerNorm.bias', 'text_model.model.encoder.layer.10.attention.attn.q.weight', 'text_model.model.encoder.layer.10.attention.attn.q.bias', 'text_model.model.encoder.layer.10.attention.attn.k.weight', 'text_model.model.encoder.layer.10.attention.attn.k.bias', 'text_model.model.encoder.layer.10.attention.attn.v.weight', 'text_model.model.encoder.layer.10.attention.attn.v.bias', 'text_model.model.encoder.layer.10.attention.attn.o.weight', 'text_model.model.encoder.layer.10.attention.attn.o.bias', 'text_model.model.encoder.layer.10.attention.LayerNorm.weight', 'text_model.model.encoder.layer.10.attention.LayerNorm.bias', 'text_model.model.encoder.layer.10.intermediate.dense.weight', 'text_model.model.encoder.layer.10.intermediate.dense.bias', 'text_model.model.encoder.layer.10.output.dense.weight', 'text_model.model.encoder.layer.10.output.dense.bias', 'text_model.model.encoder.layer.10.output.LayerNorm.weight', 'text_model.model.encoder.layer.10.output.LayerNorm.bias', 'text_model.model.encoder.layer.11.attention.attn.q.weight', 'text_model.model.encoder.layer.11.attention.attn.q.bias', 'text_model.model.encoder.layer.11.attention.attn.k.weight', 'text_model.model.encoder.layer.11.attention.attn.k.bias', 'text_model.model.encoder.layer.11.attention.attn.v.weight', 'text_model.model.encoder.layer.11.attention.attn.v.bias', 'text_model.model.encoder.layer.11.attention.attn.o.weight', 'text_model.model.encoder.layer.11.attention.attn.o.bias', 'text_model.model.encoder.layer.11.attention.LayerNorm.weight', 'text_model.model.encoder.layer.11.attention.LayerNorm.bias', 'text_model.model.encoder.layer.11.intermediate.dense.weight', 'text_model.model.encoder.layer.11.intermediate.dense.bias', 'text_model.model.encoder.layer.11.output.dense.weight', 'text_model.model.encoder.layer.11.output.dense.bias', 'text_model.model.encoder.layer.11.output.LayerNorm.weight', 'text_model.model.encoder.layer.11.output.LayerNorm.bias', 'text_model.model.encoder.relative_attention_bias.weight', 'text_model.model.pooler.dense.weight', 'text_model.model.pooler.dense.bias', 'vision_model.model.embeddings.cls_token', 'vision_model.model.embeddings.mask_token', 'vision_model.model.embeddings.position_embeddings', 'vision_model.model.embeddings.patch_embeddings.projection.weight', 'vision_model.model.embeddings.patch_embeddings.projection.bias', 'vision_model.model.encoder.layer.0.norm1.weight', 'vision_model.model.encoder.layer.0.norm1.bias', 'vision_model.model.encoder.layer.0.attention.attention.query.weight', 'vision_model.model.encoder.layer.0.attention.attention.query.bias', 'vision_model.model.encoder.layer.0.attention.attention.key.weight', 'vision_model.model.encoder.layer.0.attention.attention.key.bias', 'vision_model.model.encoder.layer.0.attention.attention.value.weight', 'vision_model.model.encoder.layer.0.attention.attention.value.bias', 'vision_model.model.encoder.layer.0.attention.output.dense.weight', 'vision_model.model.encoder.layer.0.attention.output.dense.bias', 'vision_model.model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.model.encoder.layer.0.norm2.weight', 'vision_model.model.encoder.layer.0.norm2.bias', 'vision_model.model.encoder.layer.0.mlp.fc1.weight', 'vision_model.model.encoder.layer.0.mlp.fc1.bias', 'vision_model.model.encoder.layer.0.mlp.fc2.weight', 'vision_model.model.encoder.layer.0.mlp.fc2.bias', 'vision_model.model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.model.encoder.layer.1.norm1.weight', 'vision_model.model.encoder.layer.1.norm1.bias', 'vision_model.model.encoder.layer.1.attention.attention.query.weight', 'vision_model.model.encoder.layer.1.attention.attention.query.bias', 'vision_model.model.encoder.layer.1.attention.attention.key.weight', 'vision_model.model.encoder.layer.1.attention.attention.key.bias', 'vision_model.model.encoder.layer.1.attention.attention.value.weight', 'vision_model.model.encoder.layer.1.attention.attention.value.bias', 'vision_model.model.encoder.layer.1.attention.output.dense.weight', 'vision_model.model.encoder.layer.1.attention.output.dense.bias', 'vision_model.model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.model.encoder.layer.1.norm2.weight', 'vision_model.model.encoder.layer.1.norm2.bias', 'vision_model.model.encoder.layer.1.mlp.fc1.weight', 'vision_model.model.encoder.layer.1.mlp.fc1.bias', 'vision_model.model.encoder.layer.1.mlp.fc2.weight', 'vision_model.model.encoder.layer.1.mlp.fc2.bias', 'vision_model.model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.model.encoder.layer.2.norm1.weight', 'vision_model.model.encoder.layer.2.norm1.bias', 'vision_model.model.encoder.layer.2.attention.attention.query.weight', 'vision_model.model.encoder.layer.2.attention.attention.query.bias', 'vision_model.model.encoder.layer.2.attention.attention.key.weight', 'vision_model.model.encoder.layer.2.attention.attention.key.bias', 'vision_model.model.encoder.layer.2.attention.attention.value.weight', 'vision_model.model.encoder.layer.2.attention.attention.value.bias', 'vision_model.model.encoder.layer.2.attention.output.dense.weight', 'vision_model.model.encoder.layer.2.attention.output.dense.bias', 'vision_model.model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.model.encoder.layer.2.norm2.weight', 'vision_model.model.encoder.layer.2.norm2.bias', 'vision_model.model.encoder.layer.2.mlp.fc1.weight', 'vision_model.model.encoder.layer.2.mlp.fc1.bias', 'vision_model.model.encoder.layer.2.mlp.fc2.weight', 'vision_model.model.encoder.layer.2.mlp.fc2.bias', 'vision_model.model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.model.encoder.layer.3.norm1.weight', 'vision_model.model.encoder.layer.3.norm1.bias', 'vision_model.model.encoder.layer.3.attention.attention.query.weight', 'vision_model.model.encoder.layer.3.attention.attention.query.bias', 'vision_model.model.encoder.layer.3.attention.attention.key.weight', 'vision_model.model.encoder.layer.3.attention.attention.key.bias', 'vision_model.model.encoder.layer.3.attention.attention.value.weight', 'vision_model.model.encoder.layer.3.attention.attention.value.bias', 'vision_model.model.encoder.layer.3.attention.output.dense.weight', 'vision_model.model.encoder.layer.3.attention.output.dense.bias', 'vision_model.model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.model.encoder.layer.3.norm2.weight', 'vision_model.model.encoder.layer.3.norm2.bias', 'vision_model.model.encoder.layer.3.mlp.fc1.weight', 'vision_model.model.encoder.layer.3.mlp.fc1.bias', 'vision_model.model.encoder.layer.3.mlp.fc2.weight', 'vision_model.model.encoder.layer.3.mlp.fc2.bias', 'vision_model.model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.model.encoder.layer.4.norm1.weight', 'vision_model.model.encoder.layer.4.norm1.bias', 'vision_model.model.encoder.layer.4.attention.attention.query.weight', 'vision_model.model.encoder.layer.4.attention.attention.query.bias', 'vision_model.model.encoder.layer.4.attention.attention.key.weight', 'vision_model.model.encoder.layer.4.attention.attention.key.bias', 'vision_model.model.encoder.layer.4.attention.attention.value.weight', 'vision_model.model.encoder.layer.4.attention.attention.value.bias', 'vision_model.model.encoder.layer.4.attention.output.dense.weight', 'vision_model.model.encoder.layer.4.attention.output.dense.bias', 'vision_model.model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.model.encoder.layer.4.norm2.weight', 'vision_model.model.encoder.layer.4.norm2.bias', 'vision_model.model.encoder.layer.4.mlp.fc1.weight', 'vision_model.model.encoder.layer.4.mlp.fc1.bias', 'vision_model.model.encoder.layer.4.mlp.fc2.weight', 'vision_model.model.encoder.layer.4.mlp.fc2.bias', 'vision_model.model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.model.encoder.layer.5.norm1.weight', 'vision_model.model.encoder.layer.5.norm1.bias', 'vision_model.model.encoder.layer.5.attention.attention.query.weight', 'vision_model.model.encoder.layer.5.attention.attention.query.bias', 'vision_model.model.encoder.layer.5.attention.attention.key.weight', 'vision_model.model.encoder.layer.5.attention.attention.key.bias', 'vision_model.model.encoder.layer.5.attention.attention.value.weight', 'vision_model.model.encoder.layer.5.attention.attention.value.bias', 'vision_model.model.encoder.layer.5.attention.output.dense.weight', 'vision_model.model.encoder.layer.5.attention.output.dense.bias', 'vision_model.model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.model.encoder.layer.5.norm2.weight', 'vision_model.model.encoder.layer.5.norm2.bias', 'vision_model.model.encoder.layer.5.mlp.fc1.weight', 'vision_model.model.encoder.layer.5.mlp.fc1.bias', 'vision_model.model.encoder.layer.5.mlp.fc2.weight', 'vision_model.model.encoder.layer.5.mlp.fc2.bias', 'vision_model.model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.model.encoder.layer.6.norm1.weight', 'vision_model.model.encoder.layer.6.norm1.bias', 'vision_model.model.encoder.layer.6.attention.attention.query.weight', 'vision_model.model.encoder.layer.6.attention.attention.query.bias', 'vision_model.model.encoder.layer.6.attention.attention.key.weight', 'vision_model.model.encoder.layer.6.attention.attention.key.bias', 'vision_model.model.encoder.layer.6.attention.attention.value.weight', 'vision_model.model.encoder.layer.6.attention.attention.value.bias', 'vision_model.model.encoder.layer.6.attention.output.dense.weight', 'vision_model.model.encoder.layer.6.attention.output.dense.bias', 'vision_model.model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.model.encoder.layer.6.norm2.weight', 'vision_model.model.encoder.layer.6.norm2.bias', 'vision_model.model.encoder.layer.6.mlp.fc1.weight', 'vision_model.model.encoder.layer.6.mlp.fc1.bias', 'vision_model.model.encoder.layer.6.mlp.fc2.weight', 'vision_model.model.encoder.layer.6.mlp.fc2.bias', 'vision_model.model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.model.encoder.layer.7.norm1.weight', 'vision_model.model.encoder.layer.7.norm1.bias', 'vision_model.model.encoder.layer.7.attention.attention.query.weight', 'vision_model.model.encoder.layer.7.attention.attention.query.bias', 'vision_model.model.encoder.layer.7.attention.attention.key.weight', 'vision_model.model.encoder.layer.7.attention.attention.key.bias', 'vision_model.model.encoder.layer.7.attention.attention.value.weight', 'vision_model.model.encoder.layer.7.attention.attention.value.bias', 'vision_model.model.encoder.layer.7.attention.output.dense.weight', 'vision_model.model.encoder.layer.7.attention.output.dense.bias', 'vision_model.model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.model.encoder.layer.7.norm2.weight', 'vision_model.model.encoder.layer.7.norm2.bias', 'vision_model.model.encoder.layer.7.mlp.fc1.weight', 'vision_model.model.encoder.layer.7.mlp.fc1.bias', 'vision_model.model.encoder.layer.7.mlp.fc2.weight', 'vision_model.model.encoder.layer.7.mlp.fc2.bias', 'vision_model.model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.model.encoder.layer.8.norm1.weight', 'vision_model.model.encoder.layer.8.norm1.bias', 'vision_model.model.encoder.layer.8.attention.attention.query.weight', 'vision_model.model.encoder.layer.8.attention.attention.query.bias', 'vision_model.model.encoder.layer.8.attention.attention.key.weight', 'vision_model.model.encoder.layer.8.attention.attention.key.bias', 'vision_model.model.encoder.layer.8.attention.attention.value.weight', 'vision_model.model.encoder.layer.8.attention.attention.value.bias', 'vision_model.model.encoder.layer.8.attention.output.dense.weight', 'vision_model.model.encoder.layer.8.attention.output.dense.bias', 'vision_model.model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.model.encoder.layer.8.norm2.weight', 'vision_model.model.encoder.layer.8.norm2.bias', 'vision_model.model.encoder.layer.8.mlp.fc1.weight', 'vision_model.model.encoder.layer.8.mlp.fc1.bias', 'vision_model.model.encoder.layer.8.mlp.fc2.weight', 'vision_model.model.encoder.layer.8.mlp.fc2.bias', 'vision_model.model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.model.encoder.layer.9.norm1.weight', 'vision_model.model.encoder.layer.9.norm1.bias', 'vision_model.model.encoder.layer.9.attention.attention.query.weight', 'vision_model.model.encoder.layer.9.attention.attention.query.bias', 'vision_model.model.encoder.layer.9.attention.attention.key.weight', 'vision_model.model.encoder.layer.9.attention.attention.key.bias', 'vision_model.model.encoder.layer.9.attention.attention.value.weight', 'vision_model.model.encoder.layer.9.attention.attention.value.bias', 'vision_model.model.encoder.layer.9.attention.output.dense.weight', 'vision_model.model.encoder.layer.9.attention.output.dense.bias', 'vision_model.model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.model.encoder.layer.9.norm2.weight', 'vision_model.model.encoder.layer.9.norm2.bias', 'vision_model.model.encoder.layer.9.mlp.fc1.weight', 'vision_model.model.encoder.layer.9.mlp.fc1.bias', 'vision_model.model.encoder.layer.9.mlp.fc2.weight', 'vision_model.model.encoder.layer.9.mlp.fc2.bias', 'vision_model.model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.model.encoder.layer.10.norm1.weight', 'vision_model.model.encoder.layer.10.norm1.bias', 'vision_model.model.encoder.layer.10.attention.attention.query.weight', 'vision_model.model.encoder.layer.10.attention.attention.query.bias', 'vision_model.model.encoder.layer.10.attention.attention.key.weight', 'vision_model.model.encoder.layer.10.attention.attention.key.bias', 'vision_model.model.encoder.layer.10.attention.attention.value.weight', 'vision_model.model.encoder.layer.10.attention.attention.value.bias', 'vision_model.model.encoder.layer.10.attention.output.dense.weight', 'vision_model.model.encoder.layer.10.attention.output.dense.bias', 'vision_model.model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.model.encoder.layer.10.norm2.weight', 'vision_model.model.encoder.layer.10.norm2.bias', 'vision_model.model.encoder.layer.10.mlp.fc1.weight', 'vision_model.model.encoder.layer.10.mlp.fc1.bias', 'vision_model.model.encoder.layer.10.mlp.fc2.weight', 'vision_model.model.encoder.layer.10.mlp.fc2.bias', 'vision_model.model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.model.encoder.layer.11.norm1.weight', 'vision_model.model.encoder.layer.11.norm1.bias', 'vision_model.model.encoder.layer.11.attention.attention.query.weight', 'vision_model.model.encoder.layer.11.attention.attention.query.bias', 'vision_model.model.encoder.layer.11.attention.attention.key.weight', 'vision_model.model.encoder.layer.11.attention.attention.key.bias', 'vision_model.model.encoder.layer.11.attention.attention.value.weight', 'vision_model.model.encoder.layer.11.attention.attention.value.bias', 'vision_model.model.encoder.layer.11.attention.output.dense.weight', 'vision_model.model.encoder.layer.11.attention.output.dense.bias', 'vision_model.model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.model.encoder.layer.11.norm2.weight', 'vision_model.model.encoder.layer.11.norm2.bias', 'vision_model.model.encoder.layer.11.mlp.fc1.weight', 'vision_model.model.encoder.layer.11.mlp.fc1.bias', 'vision_model.model.encoder.layer.11.mlp.fc2.weight', 'vision_model.model.encoder.layer.11.mlp.fc2.bias', 'vision_model.model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.model.layernorm.weight', 'vision_model.model.layernorm.bias', 'vlhead.logit_scale', 'vlhead.logit_bias', 'vlhead.vision_mapping_network.weight', 'vlhead.vision_mapping_network.bias', 'vlhead.text_mapping_network.weight', 'vlhead.text_mapping_network.bias', 'vlhead.vision_layer_norm.weight', 'vlhead.vision_layer_norm.bias', 'vlhead.text_layer_norm.weight', 'vlhead.text_layer_norm.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
